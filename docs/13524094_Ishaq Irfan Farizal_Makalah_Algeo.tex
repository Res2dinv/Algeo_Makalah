\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue
}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{subcaption}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Implementation of Reinhard Method Using Orthogonal Vector Basis $l\alpha\beta$ for Color Transfer: Analysis of Success and Failure Modes}

\author{\IEEEauthorblockN{Ishaq Irfan}
\IEEEauthorblockA{\textit{School of Electrical Engineering and Informatics} \\
\textit{Bandung Institute of Technology}\\
Bandung, Indonesia \\
13524094@mahasiswa.itb.ac.id\\
ishaqirfanfrzl@gmail.com}
}

\maketitle

\begin{abstract}
This paper discusses the application of Linear Algebra concepts, especially change of basis and vector orthogonality, in digital image processing. The Reinhard method is used to transfer color statistics from a target image to a source image. The main focus of this research is to prove the effectiveness of the $l\alpha\beta$ color space which has orthogonal vector basis to minimize correlation between color channels, unlike the RGB space which is highly correlated. Through systematic experiments across nine source-target combinations, we analyze both successful transfers and failure modes, revealing that success depends critically on structural similarity, target color variation, and semantic compatibility. Experimental results show that transformation to orthogonal basis successfully decorrelates the data for most images (92-94\% correlation reduction), but this mathematical correctness does not guarantee perceptually acceptable results when global statistics are applied to semantically complex scenes.
\end{abstract}

\begin{IEEEkeywords}
Linear Algebra, Vector Space, Orthogonal Basis, Color Transfer, Reinhard, Basis Transformation, Covariance Analysis.
\end{IEEEkeywords}

\section{Introduction}

\subsection{Background}
Color is a fundamental feature in digital image processing and computer vision, as it strongly influences how a scene is perceived. A digital image represents visual information through spatial sampling and light intensity quantization. In practice, images are commonly stored in the RGB (Red, Green, Blue) color space, where each pixel is described as a combination of three primary color intensities. While this representation is convenient for image acquisition and display, it is not always suitable for analytical color manipulation.

One task that highlights this limitation is color transfer, where color statistics from a reference image are applied to another image. In applications such as photography and film, this technique is often used to approximate a change in atmosphere, for example by making an image appear darker or more "night-like" by borrowing colors from a night scene.

Applying color transfer directly in RGB space often leads to hard to predict results. Although the RGB axes are mathematically orthogonal, the channels are statistically correlated in natural images. As a consequence, modifying one channel can unintentionally affect brightness or color balance, making controlled manipulation difficult.

To better study and perform color transfer, it is useful to transform image data into a different vector space where luminance and chromatic information are more clearly separated. From a linear algebra perspective, this corresponds to a change of basis that aims to reduce inter-channel correlation and simplify statistical analysis.

Reinhard et al. (2001)\cite{reinhard2001} proposed a method that transforms RGB values into the $l\alpha\beta$ color space, which is constructed using logarithmic transformations and principal component analysis of natural images. In this space, the channels are approximately decorrelated, allowing simple operations such as matching the mean and standard deviation of each channel between images.

However, it is important to note that this approach does not truly understand scene semantics or lighting conditions. The resulting "atmosphere change" is purely statistical: color distributions from one image are imposed onto another. As a result, effects such as day-to-night transformation may resemble applying a global color filter rather than physically accurate illumination changes.

This paper focuses on analyzing both the mathematical effectiveness and practical limitations of this method. The goal is to examine the linear algebraic transformations involved, analyze channel decorrelation through covariance analysis, demonstrate how statistical operations in an alternative basis can change image appearance, and critically evaluate when and why the method fails.

\section{Literature Review}

\subsection{Human Visual System and Color Space}
The human eye has three types of cone cells that are sensitive to different light wavelengths: \textit{Long} (L), \textit{Medium} (M), and \textit{Short} (S). Ruderman et al. \cite{ruderman1998} found that the responses of these cells have very high correlation due to overlapping sensitivity spectra. To process visual information efficiently, the brain "decorrelates" these signals into three independent perceptual pathways: light-dark pathway (luminance), red-green pathway, and yellow-blue pathway. The $l\alpha\beta$ color space is designed to mathematically mimic this neural processing mechanism.

\subsection{Linear Algebra in Image Processing}
Digital images are often treated only as 2D arrays, but in the context of color processing, images are more precisely viewed as a set of vectors in space $\mathbb{R}^n$. Basic operations such as color rotation, saturation, and \textit{grayscale} conversion are basically linear transformations $T(\mathbf{v}) = M\mathbf{v}$. Choosing the right basis $M$ greatly determines the ease and quality of processing results. Orthogonal basis such as those used in JPEG compression (DCT basis) or Lab color space are chosen because of their ability to minimize data redundancy.

\subsection{Related Work}
While the Reinhard method provides an elegant global solution, subsequent research has addressed its limitations. Pitié et al. \cite{pitie2007} proposed probability distribution matching in local neighborhoods to handle multi-region scenes. Tai et al. \cite{tai2005} combined segmentation with statistical transfer to preserve semantic boundaries. More recently, deep learning approaches \cite{gatys2016} have enabled content-aware style transfer that respects object semantics. These developments highlight that color statistics alone, while mathematically sound, cannot fully capture the complexity of natural image structure.

\subsection{Objectives}
The objectives of this research are:
\begin{enumerate}
    \item Implement manual linear transformation (RGB to LMS to $l\alpha\beta$).
    \item Analyze image covariance matrix to validate orthogonality properties.
    \item Systematically evaluate transfer quality across diverse source-target pairs.
    \item Identify and characterize failure modes to understand method limitations.
\end{enumerate}

\section{Theory}

\subsection{Digital Image as Vectors in Euclidean Space}
Digital images in computers are represented as pixel grids. In color images, each pixel is not just a point, but a vector in Euclidean space $\mathbb{R}^3$. If we review the RGB color model, each pixel $p$ can be written as a column vector:
\begin{equation}
\mathbf{v} = \begin{bmatrix} r \\ g \\ b \end{bmatrix} \in \mathbb{R}^3
\end{equation}
Where $r, g, b$ represent the intensity of Red, Green, and Blue channels. The image itself is a set of these vectors.

The concept of distance between colors, which is crucial in clustering algorithms like K-Means, is calculated using Euclidean Distance between two vectors $\mathbf{v}_1$ and $\mathbf{v}_2$:
\begin{equation}
d(\mathbf{v}_1, \mathbf{v}_2) = \|\mathbf{v}_1 - \mathbf{v}_2\| = \sqrt{(r_1 - r_2)^2 + (g_1 - g_2)^2 + (b_1 - b_2)^2}
\end{equation}

\subsection{Linear Transformation and Matrix Multiplication}
Color manipulation between color spaces involves linear transformation, which is a mapping function $T: \mathbb{R}^3 \to \mathbb{R}^3$ that preserves vector addition and scalar multiplication operations. In digital computation implementation, this transformation is efficiently represented using matrix multiplication.

\subsubsection{RGB to LMS Conversion}
The first step in the Reinhard algorithm is to convert the RGB basis to LMS (\textit{Long, Medium, Short}) basis. This basis simulates the spectral response of three types of cone cells in the human retina. This transformation is done by multiplying the pixel vector $\mathbf{v}_{RGB}$ with a transformation matrix $M_{RGB \to LMS}$. The elements of this matrix are determined based on visual perception measurements and color matching experiments.
\begin{equation}
\mathbf{v}_{LMS} = M_{RGB \to LMS} \cdot \mathbf{v}_{RGB}
\end{equation}
Explicitly:
\begin{equation}
\begin{bmatrix} L \\ M \\ S \end{bmatrix} = 
\begin{bmatrix} 
0.3811 & 0.5783 & 0.0402 \\ 
0.1967 & 0.7244 & 0.0782 \\ 
0.0241 & 0.1288 & 0.8444 
\end{bmatrix} 
\begin{bmatrix} R \\ G \\ B \end{bmatrix} 
\end{equation}
The matrix above has non-zero determinant, so it is \textit{invertible} (has an inverse). Matrix inverse is needed at the final reconstruction stage. Note that this matrix is not diagonal, which indicates that in LMS space too, information between channels is still correlated (mixed).

\subsubsection{Transformation to Logarithmic Space}
The distribution of light intensity data in natural images is generally \textit{skewed}. Most simple statistical algorithms, including Reinhard, assume normally distributed (Gaussian) data. Therefore, a logarithmic non-linear transformation is applied:
\begin{equation}
\mathbf{L}' = \log_{10}(L), \quad \mathbf{M}' = \log_{10}(M), \quad \mathbf{S}' = \log_{10}(S)
\end{equation}
This step transforms the data distribution to approach a bell shape (Gaussian), which makes \textit{Mean} and \textit{Standard Deviation} parameters valid and robust data descriptors.

\subsubsection{LMS to $l\alpha\beta$ Conversion (Decorrelation)}
This is the crucial stage where orthogonality concept is applied. Data in LMS space has very high correlation (because L, M, and S cell sensitivity overlaps). We want to rotate the coordinate axes such that the new axes point to the direction of largest data variance and are perpendicular to each other (Principal Component Analysis).
Transformation is done to three new channels:
\begin{itemize}
    \item $l$: Achromatic (Intensity/Luminance) $\approx L+M+S$
    \item $\alpha$: Opponent Yellow-Blue $\approx L+M-2S$
    \item $\beta$: Opponent Red-Green $\approx L-M$
\end{itemize}
Transformation matrix $M_{LMS \to l\alpha\beta}$ is constructed as follows:
\begin{equation}
\begin{bmatrix} l \\ \alpha \\ \beta \end{bmatrix} = 
\begin{bmatrix} 
\frac{1}{\sqrt{3}} & 0 & 0 \\ 
0 & \frac{1}{\sqrt{6}} & 0 \\ 
0 & 0 & \frac{1}{\sqrt{2}} 
\end{bmatrix} 
\begin{bmatrix} 
1 & 1 & 1 \\ 
1 & 1 & -2 \\ 
1 & -1 & 0 
\end{bmatrix} 
\begin{bmatrix} \mathbf{L}' \\ \mathbf{M}' \\ \mathbf{S}' \end{bmatrix}
\end{equation}
The first matrix is a normalization matrix to maintain vector length, while the second matrix performs basis rotation.

\subsection{Mathematical Analysis of Orthogonality}
Let's review the rows of the rotation matrix (before normalization) as new basis vectors:
$r_1 = [1, 1, 1]$, $r_2 = [1, 1, -2]$, and $r_3 = [1, -1, 0]$.
To prove that this basis is orthogonal (mutually perpendicular), we calculate the dot product between rows:
\begin{equation}
r_1 \cdot r_2 = (1)(1) + (1)(1) + (1)(-2) = 1 + 1 - 2 = 0
\end{equation}
\begin{equation}
r_1 \cdot r_3 = (1)(1) + (1)(-1) + (1)(0) = 1 - 1 + 0 = 0
\end{equation}
\begin{equation}
r_2 \cdot r_3 = (1)(1) + (1)(-1) + (-2)(0) = 1 - 1 + 0 = 0
\end{equation}
Because all dot products are zero, the basis set $\{r_1, r_2, r_3\}$ is proven to be orthogonal. This property validates that information on channels $l, \alpha,$ and $\beta$ is truly mathematically separated. Modifying channel $l$ (light-dark) will not ``leak'' and change color nuance on channels $\alpha$ or $\beta$, which is the key to this method's theoretical advantage.

\subsection{Vector Statistics (Mean \& Standard Deviation)}
The Reinhard method bases color transfer on matching color vector distribution statistics. We calculate the ``center point'' (Mean) and ``spread'' (Standard Deviation) of the pixel point cloud in vector space.
\begin{itemize}
    \item Mean Vector ($\boldsymbol{\mu}$):
    \begin{equation}
    \boldsymbol{\mu} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{v}_i
    \end{equation}
    \item Standard Deviation ($\boldsymbol{\sigma}$):
    \begin{equation}
    \boldsymbol{\sigma} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (\mathbf{v}_i - \boldsymbol{\mu})^2}
    \end{equation}
\end{itemize}
(Square and root operations are done \textit{element-wise} for each channel).

\section{Methodology}

\subsection{System Flow}
This research implements the global Reinhard color transfer algorithm. This approach is chosen to prove that proper basis transformation to $l\alpha\beta$ space is sufficient to separate color correlation and enable effective image atmosphere manipulation without complex segmentation.

\subsection{Step-by-Step Algorithm}
Here are the procedural details of the implemented algorithm:

\subsubsection{Step 1: Input and Pre-processing}
The system receives two input images: Source Image ($I_s$) whose color will be modified, and Target Image ($I_t$) which becomes the color reference. Both images are normalized to range $[0, 1]$ to maintain consistency of \texttt{float} matrix operations.

\subsubsection{Step 2: Transformation to $l\alpha\beta$ Space}
Images $I_s$ and $I_t$ are converted from RGB space to orthogonal space $l\alpha\beta$ through two stages of linear transformation and one stage of non-linear (logarithmic) as explained in Section III. The result is image tensor in $l\alpha\beta$ space:
\begin{align}
\mathbf{V}_s &= \text{RGBtoLAB}(I_s) \\
\mathbf{V}_t &= \text{RGBtoLAB}(I_t)
\end{align}
Where $\mathbf{V}_s, \mathbf{V}_t \in \mathbb{R}^{H \times W \times 3}$.

\subsubsection{Step 3: Statistical Analysis}
For each channel $k \in \{l, \alpha, \beta\}$, mean ($\mu$) and standard deviation ($\sigma$) are calculated from all pixels in the image. This reduces complex image color information to a compact statistical vector representation.
\begin{align}
\mu_{s,k} &= \frac{1}{N} \sum_{i=1}^N v_{s,k}^{(i)}, \quad \sigma_{s,k} = \sqrt{\frac{1}{N} \sum_{i=1}^N (v_{s,k}^{(i)} - \mu_{s,k})^2} \\
\mu_{t,k} &= \frac{1}{N} \sum_{i=1}^N v_{t,k}^{(i)}, \quad \sigma_{t,k} = \sqrt{\frac{1}{N} \sum_{i=1}^N (v_{t,k}^{(i)} - \mu_{t,k})^2}
\end{align}

\subsubsection{Step 4: Statistical Transfer}
Each pixel in source image ($v_{s,k}$) is transformed so that its statistical distribution matches the target image. This process involves shifting to zero point (subtracting by source mean), scaling with standard deviation ratio, and shifting back to target mean position:
\begin{equation}
v_{out,k} = \left( v_{s,k} - \mu_{s,k} \right) \frac{\sigma_{t,k}}{\sigma_{s,k}} + \mu_{t,k}
\end{equation}
This operation is done separately for channels $l, \alpha,$ and $\beta$.

\subsubsection{Step 5: Reconstruction (Inverse Transformation)}
Result image ($V_{out}$) is returned to RGB space using inverse of transformation matrices.
\begin{align}
\mathbf{V}_{LMS}' &= \mathbf{V}_{out} \cdot M_{l\alpha\beta \to LMS} \\
\mathbf{V}_{LMS} &= 10^{\mathbf{V}_{LMS}'} \quad (\text{Inverse Log}) \\
I_{out} &= \mathbf{V}_{LMS} \cdot M_{LMS \to RGB}
\end{align}
Final result $I_{out}$ is then \textit{clipped} to range $[0, 1]$ for visual display.


\section{Visual Results}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.3\linewidth]{../code/source/source1_fuji.png}
\includegraphics[width=0.3\linewidth]{../code/source/source2_forest.png}
\includegraphics[width=0.3\linewidth]{../code/source/source3_people.png}
\caption{Source Images. From left to right: Source 1 (Fuji) \cite{img:fuji}, Source 2 (Forest) \cite{img:forest}, Source 3 (People) \cite{img:people}.}
\label{fig:source_dataset}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.3\linewidth]{../code/target/target1_deepsea.png}
\includegraphics[width=0.3\linewidth]{../code/target/target2_darknight.png}
\includegraphics[width=0.3\linewidth]{../code/target/target3_sunset.png}
\caption{Target Images. From left to right: Target 1 (Deepsea) \cite{img:deepsea}, Target 2 (Darknight) \cite{img:darknight}, Target 3 (Sunset) \cite{img:sunset}.}
\label{fig:target_dataset}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.24\linewidth]{../code/source/source1_fuji.png}
\includegraphics[width=0.24\linewidth]{../code/target/target1_deepsea.png}
\includegraphics[width=0.24\linewidth]{../code/output/result_source1_fuji_TO_target1_deepsea.jpg}
\includegraphics[width=0.24\linewidth]{../code/output/result_source1_fuji_TO_target2_darknight.jpg}
\caption{Source 1 (Fuji) \cite{img:fuji} transfers. From left: Original, Target 1 (Deepsea) \cite{img:deepsea}, Result s1t1 (Success), Result s1t2 (Failure - statistical collapse)}
\label{fig:fuji_success_fail}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.24\linewidth]{../code/source/source3_people.png}
\includegraphics[width=0.24\linewidth]{../code/target/target3_sunset.png}
\includegraphics[width=0.24\linewidth]{../code/output/result_source3_people_TO_target3_sunset.jpg}
\includegraphics[width=0.24\linewidth]{../code/output/result_source3_people_TO_target2_darknight.jpg}
\caption{Source 3 (People) \cite{img:people} transfers. From left: Original, Target 3 (Sunset) \cite{img:sunset}, Result s3t3 (Semantic failure), Result s3t2 (Extreme darkening)}
\label{fig:people_failures}
\end{figure*}

\begin{figure*}[htbp]
\includegraphics[width=0.24\linewidth]{../code/source/source2_forest.png}
\includegraphics[width=0.24\linewidth]{../code/output/result_source2_forest_TO_target1_deepsea.jpg}
\includegraphics[width=0.24\linewidth]{../code/output/result_source2_forest_TO_target2_darknight.jpg}
\includegraphics[width=0.24\linewidth]{../code/output/result_source2_forest_TO_target3_sunset.jpg}
\caption{Forest (Source 2) \cite{img:forest} transfers show consistent failure across all targets due to multimodal color distribution}
\label{fig:forest_all}
\end{figure*}

\clearpage

\section{Analysis of Results}


\subsection{Quantitative Analysis of Color Space Transformation}

The covariance analysis in Table~\ref{tab:covariance} reveals the effectiveness of the $l\alpha\beta$ transformation in decorrelating color channels:

\begin{table}[h]
\centering
\caption{Off-diagonal covariance comparison between RGB and $l\alpha\beta$ spaces}
\label{tab:covariance}
\begin{tabular}{lcc}
\hline
\textbf{Image} & RGB Space & $l\alpha\beta$ Space \\
\hline
Source 1 (Fuji) & 0.015136 & 0.001166 \\
Source 2 (Forest) & 0.023603 & 0.030573 \\
Source 3 (People) & 0.057476 & 0.003200 \\
\hline
\end{tabular}
\end{table}

For Sources 1 and 3, the transformation achieves significant decorrelation (92-94\% reduction in off-diagonal correlation). However, Source 2 shows an \textit{increase} in correlation (0.024 $\rightarrow$ 0.031), suggesting that images with complex spectral characteristics (multiple greens, browns, shadowing) may not benefit equally from the orthogonal transformation. This is the first indication that the method's effectiveness is content-dependent.

In Source 1 (RGB space), calculation results show:
\begin{equation}
C_{RGB} = \begin{bmatrix} 
0.0179 & 0.0176 & 0.0155 \\ 
0.0176 & 0.0293 & 0.0350 \\ 
0.0155 & 0.0350 & 0.0470 
\end{bmatrix}
\end{equation}
Note that non-diagonal element values are significant compared to diagonal elements (variance), showing information redundancy. After transformation to $l\alpha\beta$ space:
\begin{equation}
C_{l\alpha\beta} = \begin{bmatrix} 
0.0648 & -0.0033 & -0.0015 \\ 
-0.0033 & 0.0017 & 0.0004 \\ 
-0.0015 & 0.0004 & 0.0001 
\end{bmatrix}
\end{equation}
Non-diagonal elements approach zero (order $10^{-3}$ to $10^{-4}$), indicating the matrix approaches a diagonal matrix, verifying successful decorrelation for this image type.

\subsection{Result Explanation: Successful Transfer}

The s1t1 transfer (shown in Figure \ref{fig:fuji_success_fail}, third image) produced visually coherent results because:

\begin{enumerate}
    \item Structural compatibility: Both scenes feature large, uniform regions (sky/water, mountain/underwater formations) with smooth gradients. Global statistics meaningfully describe both images.
    
    \item Appropriate luminance mapping: 
    \begin{align}
    \mu_{\text{source}}^{l} &= -0.431 \rightarrow \mu_{\text{target}}^{l} = -1.740 \\
    \sigma_{\text{source}}^{l} &= 0.255 \rightarrow \sigma_{\text{target}}^{l} = 0.404
    \end{align}
    The transfer darkens the scene while increasing contrast, perceptually matching the transition from daylight to deep underwater lighting.
    
    \item Preserved chromatic relationships: Blue sky maps to blue-green water tones, and mountain earth tones shift to darker underwater rock colors. Similar semantic structure preserves recognizability.
\end{enumerate}

\subsection{Result Explanation: Failure Mode 1 (Extreme Statistics)}

The s1t2 transfer (Fuji $\rightarrow$ dark night, Figure \ref{fig:fuji_success_fail}, fourth image) produces an almost completely black image due to \textit{statistical collapse}:

\begin{align}
\mu_{\text{target}}^{\alpha} &= 7.64 \times 10^{-4} \\
\sigma_{\text{target}}^{\alpha} &= 6.85 \times 10^{-7} \quad \text{(effectively zero)}
\end{align}

When the target has near-zero standard deviation, the transfer formula
\begin{equation}
\text{result}_{lab} = \left(\text{source}_{lab} - \mu_{\text{source}}\right) \cdot \frac{\sigma_{\text{target}}}{\sigma_{\text{source}} + \epsilon} + \mu_{\text{target}}
\end{equation}
multiplies all source variation by $\approx 10^{-7}$, compressing the image to uniform darkness. The mathematical cause:
\begin{equation}
\text{scaling factor} = \frac{\sigma_{\text{target}}^{\alpha}}{\sigma_{\text{source}}^{\alpha}} = \frac{6.85 \times 10^{-7}}{0.041} \approx 1.67 \times 10^{-5}
\end{equation}
Every pixel's chromatic variation is multiplied by $0.0000167$, removing all color distinctions. This reveals a critical limitation: the method assumes meaningful target color variation. For targets with minimal dynamic range (night scenes, monochromatic images), transfer becomes ineffective.

\subsection{Result Explanation: Failure Mode 2 (Semantic Mismatch)}

The s3t3 transfer (Figure \ref{fig:people_failures}, third image) demonstrates fundamental semantic blindness:

\begin{itemize}
    \item Unnatural colorization: Skin, grass, and sky receive identical global transformation, resulting in orange-tinted skin (matching sunset) while sky retains blue patches.
    
    \item Loss of local coherence: The sunset target has high $\beta$ variance (red-green axis = 0.065), applied globally:
    \begin{align}
    \mu_{\text{target}}^{\beta} &= -0.210 \quad \text{(strong red-green shift)} \\
    \sigma_{\text{target}}^{\beta} &= 0.261 \quad \text{(high variation)}
    \end{align}
    These describe the sunset gradient (dark ground to orange sky), but global application creates color ``patches'' respecting no object boundaries.
    
    \item Perceptual failure: While mathematically correct (output statistics match target), results are unacceptable because humans expect semantic consistency.
\end{itemize}

This highlights that color is not independent of content, a core assumption violated by global methods.

\subsection{Result Explanation: Failure Mode 3 (Complex Scene)}

The forest source (Figure \ref{fig:forest_all}) performed poorly across all targets because:

\begin{enumerate}
    \item Failed decorrelation: Forest's $l\alpha\beta$ covariance (0.031) indicates transformation didn't decorrelate channels. Complex spectral mixtures (tree trunks, moss, shadows, foliage) resist simple basis transformations.
    
    \item Multimodal distributions: Forest scenes have multiple color clusters (dark shadows, bright foliage, brown bark). Global mean/std captures none accurately, producing muddy transfers.
    
    \item Detail loss: Fine texture depends on local contrast. Global std scaling cannot preserve details with different target contrast.
\end{enumerate}

\section{Discussion}

\subsection{Mathematical Correctness vs. Perceptual Quality}
Our experiments reveal a crucial disconnect: mathematical decorrelation (92-94\% for Sources 1 and 3) does not guarantee perceptually acceptable results. The orthogonal transformation successfully separates statistical dependencies, as proven by near-diagonal covariance matrices, yet this separation operates at a global level blind to semantic content.

\subsection{Comparison to Modern Approaches}
Our findings explain why subsequent methods moved beyond global statistics:
\begin{itemize}
    \item Local transfer methods \cite{pitie2007} address multi-region problems through neighborhood-based statistics
    \item Segmentation-based methods\cite{tai2005} transfer statistics per semantic segment
    \item Deep learning approaches \cite{gatys2016} learn content-aware transformations respecting object boundaries
\end{itemize}

The Reinhard method's elegance, three matrix multiplications and basic statistics, comes at the cost of semantic awareness. For 2001, this trade-off was reasonable. Today, it serves as a foundational technique highlighting why content-aware processing is essential.

\subsection{Practical Recommendations}
For practitioners considering the Reinhard method:
\begin{enumerate}
    \item Pre-screen pairs: Calculate $\sigma_{\text{target}}$. If any $\sigma < 0.01$, expect poor results.
    \item Use for re-lighting, not re-coloring: Excels at brightness/mood adjustment for similar scenes.
    \item Consider scene complexity: Simple landscapes work; portraits and textured scenes often fail.
    \item Combine with masking: Manually segment important subjects, transfer statistics separately for foreground/background.
\end{enumerate}

\subsection{Limitations of Global Approach}
Although computationally efficient ($\mathcal{O}(N)$), the global method assumes unimodal color distributions. For images with complex compositions (bright foreground contrasting with background), global transfer may flatten nuances. However, for atmospheric simulation (weather, time effects), the global approach provides visually coherent results without boundary artifacts from segmentation.

\section{Conclusion}

This research successfully demonstrates Linear Algebra concepts, particularly change of basis and vector orthogonality, in digital image processing through the Reinhard color transfer method. Transformation to $l\alpha\beta$ space effectively decorrelates channels for most natural images (92-94\% correlation reduction), as validated through covariance analysis. This orthogonality enables independent luminance and chromaticity manipulation.

However, systematic evaluation across nine source-target combinations reveals that mathematical decorrelation alone cannot guarantee perceptual success. The method succeeds when source and target share structural similarity (s1t1: landscape to landscape), but fails significantly when targets have extreme statistics (s1t2, s3t2: $\sigma \approx 10^{-7}$ causing statistical collapse) or when semantic content mismatches (s3t3: orange-tinted skin from sunset transfer).

These failures are not implementation flaws but fundamental limitations of any global statistical approach. Color statistics cannot capture image semantics, a lesson driving two decades of research toward content-aware methods. Nevertheless, for appropriate use cases (re-lighting structurally similar scenes), the Reinhard method remains effective, requiring only seconds of computation without training data. Its value today is educational: demonstrating why modern computer vision requires semantic understanding beyond mathematical elegance.

\section*{Acknowledgment}
The author thanks the IF2123 Linear Algebra and Geometry course instructors at Bandung Institute of Technology for guidance in this research.

\begin{thebibliography}{00}

\bibitem{reinhard2001}
E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley,
``Color transfer between images,''
\textit{IEEE Computer Graphics and Applications}, vol. 21, no. 5, pp. 34--41, 2001.

\bibitem{ruderman1998}
D. L. Ruderman, T. W. Cronin, and C. C. Chiao,
``Statistics of cone responses to natural images: implications for visual coding,''
\textit{Journal of the Optical Society of America A}, vol. 15, no. 8, pp. 2036--2045, 1998.

\bibitem{pitie2007}
F. Pitié, A. C. Kokaram, and R. Dahyot,
``Automated colour grading using colour distribution transfer,''
\textit{Computer Vision and Image Understanding}, vol. 107, no. 1-2, pp. 123--137, 2007.

\bibitem{tai2005}
Y.-W. Tai, J. Jia, and C.-K. Tang,
``Local color transfer via probabilistic segmentation by expectation-maximization,''
\textit{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, vol. 1, pp. 747--754, 2005.

\bibitem{gatys2016}
L. A. Gatys, A. S. Ecker, and M. Bethge,
``Image style transfer using convolutional neural networks,''
\textit{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp. 2414--2423, 2016.

\bibitem{img:fuji} 
``View of Mount Fuji from Ōwakudani,''
Wikimedia Commons, 2021.
Available: \url{https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/View_of_Mount_Fuji_from_%C5%8Cwakudani_20211202.jpg/1280px-View_of_Mount_Fuji_from_%C5%8Cwakudani_20211202.jpg}
[Accessed: Dec. 9, 2025, 10:23]

\bibitem{img:forest} 
A. Kumler (Cascadia Wildlands),
``Ancient forest scene,''
Environment America.
Available: \url{https://environmentamerica.org/articles/save-americas-oldest-forests/}
[Accessed: Dec. 9, 2025, 10:35]

\bibitem{img:people} 
``Children playing on sand dunes,''
Club 950 Cornwall.
Available: \url{https://club950.co.uk/}
[Accessed: Dec. 9, 2025, 10:47]

\bibitem{img:deepsea} 
National Oceanic and Atmospheric Administration,
``Underwater exploration at Northeast Canyons and Seamounts Marine Monument,''
U.S. Fish and Wildlife Service.
Available: \url{https://www.fws.gov/media/noaa-underwater-exploration-vessel}
[Accessed: Dec. 9, 2025, 11:02]

\bibitem{img:darknight} 
``Dark night scene,''
PeakPX.
Available: \url{https://www.peakpx.com/en/hd-wallpaper-desktop-fbgfq}
[Accessed: Dec. 9, 2025, 11:15]

\bibitem{img:sunset} 
``Dark sunset landscape,''
Freepik.
Available: \url{https://www.freepik.com/free-photos-vectors/dark-sunset}
[Accessed: Dec. 9, 2025, 11:28]

\end{thebibliography}

\section*{Appendices}
    \begin{itemize}
        \item GitHub Repository: \url{https://github.com/Res2dinv/Algeo_Makalah.git}
    \end{itemize}


\end{document}